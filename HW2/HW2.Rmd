---
title: "DataAn3_SalgaB"
output:
  html_document: default
---

```{r setup, include=FALSE}

rm(list=ls())

#Packages
library(tidyverse)
library(caret)
library(skimr)
library(grid)
library(glmnet)
library(ranger)
library(pdp)
library(cowplot)
library(modelsummary)
library(fixest)
library(units)
library(gbm)
library(rattle)
library(fastDummies)
library(stringr)

df_syd <- read.csv("c:/Users/benja/ceu/DataAn3/HW2/listings.csv") %>% ##########################Github
  mutate_if(is.character, factor)

```

## Introduction
This analysis aims to help a company's new, currently not operating small and mid-size apartments price setting. In the first part, the dataset will be clean and I will create some variables to make the analysis clearer and easier. I have to check what kind of factors have effects on the price of an apartment. To do that, many different methods (OLS, LASSO, Random forest) will be raced to see which performs better and its results have a better fit on the dataset.
For the sake of the analysis, the Airbnb dataset was used that includes information on many thousands of apartments in and around Sydney. In this analysis, I build a price prediction model to help the company.


## Data cleaning
The data had been downloaded free from http://insideairbnb.com/get-the-data.html. Only the Sydney dataset was downloaded and used. After the data has been loaded, I checked where are NA values were in the dataset. Sixteen variables had NA values. If in a column was less than 10 NA values, I removed them: neighborhood_overview, host_listings_count, host_total_listings_count. I also removed those variables that had nothing to do with the current analysis (different names and URLs). Other NA values in different variables were replaced with their mean values since these are probably missing values and not 0. So, for the analysis, I changed them to their mean values. Some variables only had NA values. I removed those entirely as well.
In the second part, I started to create variables for the analysis. I created the number of days since the apartment is on the market, and also I created its logarithm and square form, to be able to analyze the effect of time. I also made the logarithm form of price and number of reviews for early data analysis.
I also created some dummy variables from different with the 'fastDummies' package, since those variables only had True and False values.
I applied filters to set apartments and apartment type houses, and also I filtered put those variables, which price was more than 1000 dollars.


```{r, echo=FALSE, include=FALSE}

# where are the missing variables
to_filter <- sapply(df_syd, function(x) sum(is.na(x)))
to_filter[to_filter > 0]

# some observation is all NA, so I remove it along with some other unnecessary observations
to_drop <- c("listing_url", "scrape_id", "last_scraped", "name", "description", "neighborhood_overview", "neighbourhood_group_cleansed", "calendar_updated", "picture_url", "host_id", "host_url", "host_name", "host_since", "host_location", "host_about", "host_thumbnail_url", "host_picture_url", "host_neighbourhood", "host_verifications", "host_has_profile_pic", "neighbourhood", "bathrooms", "bathrooms_text", "license", "review_scores_checkin", "review_scores_accuracy", "review_scores_communication")
df_syd <- df_syd%>%
  select(-one_of(to_drop))

#filter for number of guests
df_syd <- df_syd %>%
  filter(accommodates < 7) %>% 
  filter(accommodates > 1)

#Room type as factor
datasummary(room_type ~ N + Percent() , data = df_syd )
df_syd <- df_syd %>%
  mutate(f_room_type = factor(room_type))

#filter out hotel rooms
df_syd <- df_syd %>%
  filter(f_room_type %in% c("Entire home/apt", "Private room", "Shared room"))

# By many observation NA replaced with the mean expect bed (beds = accomodates)
df_syd <- df_syd %>%
  mutate(
    reviews_per_month = ifelse(is.na(reviews_per_month), median(reviews_per_month, na.rm = T), reviews_per_month),
    review_scores_rating = ifelse(is.na(review_scores_rating), median(review_scores_rating, na.rm = T), review_scores_rating),
    review_scores_location = ifelse(is.na(review_scores_location), median(review_scores_location, na.rm = T), review_scores_location),
    review_scores_cleanliness = ifelse(is.na(review_scores_cleanliness), median(review_scores_cleanliness, na.rm = T), review_scores_cleanliness),
    review_scores_value = ifelse(is.na(review_scores_value), median(review_scores_value, na.rm = T), review_scores_value),
    bedrooms = ifelse(is.na(bedrooms), median(bedrooms, na.rm = T), bedrooms),
    beds = ifelse(is.na(beds), accommodates, beds))

# remove $ from price
df_syd <- df_syd %>% 
  mutate(price = as.numeric(price))

#create new variables
df_syd <- df_syd %>%
  mutate(
    no_day_since_r = as.numeric(as.Date(calendar_last_scraped,format="%Y-%m-%d")- as.Date(first_review, format="%Y-%m-%d")),
    n_host_response_rate = as.numeric(host_response_rate),
    n_review_scores_rating = as.numeric(review_scores_rating),
    n_reviews_per_month = as.numeric(reviews_per_month),
    ln_day_since = log(no_day_since_r+1),
    ln_day_since2 = log(no_day_since_r+1)^2,
    ln_day_since3 = log(no_day_since_r+1)^3,
    no_day_since_r2 = no_day_since_r^2,
    no_day_since_r3 = no_day_since_r^3,
    ln_price = log(price),
    ln_no_review = log(number_of_reviews),
     ln_day_since=ifelse(is.na(ln_day_since),0, ln_day_since),
    ln_day_since2=ifelse(is.na(ln_day_since2),0, ln_day_since2),
    ln_day_since3=ifelse(is.na(ln_day_since3),0, ln_day_since3),
     no_day_since_r=ifelse(is.na(no_day_since_r),0, no_day_since_r),
    no_day_since_r2=ifelse(is.na(no_day_since_r2),0, no_day_since_r2),
    no_day_since_r3=ifelse(is.na(no_day_since_r3),0, no_day_since_r3))

# drop NA variables
df_syd <- df_syd %>%
  drop_na(host_listings_count,
          host_total_listings_count)

# create dummy variables
df_syd <- dummy_cols(df_syd, select_columns = c('host_is_superhost', 'host_identity_verified', 'has_availability', 'instant_bookable'), remove_selected_columns = TRUE)

df_syd <- df_syd %>%
  mutate(host_resp_days_d = ifelse(host_response_time == "a few days or more", 1,0),
         host_resp_NA_d = ifelse(host_response_time == "N/A", 1,0),
         host_resp_in_days_d = ifelse(host_response_time == "within a day", 1,0),
         host_resp_in_f_hour_d = ifelse(host_response_time == "within a few hours", 1,0),
         host_resp_in_h_d = ifelse(host_response_time == "within an hour", 1,0))

# drop unnecesary variables
to_drop <- c("room_type")
df_syd <- df_syd%>%
  select(-one_of(to_drop))

# Squares and further values to create
df_syd <- df_syd %>%
  mutate(accommodates2 = accommodates^2, 
         ln_accommodates = log(accommodates) ,
         ln_accommodates2 = log(accommodates)^2,
         ln_beds = log(beds),
         ln_number_of_reviews = log(number_of_reviews+1))

# filter for price
df_syd <- df_syd %>%
  filter(price < 1000,
         price >100)

# where do we have missing variables now?
to_filter <- sapply(df_syd, function(x) sum(is.na(x)))
to_filter[to_filter > 0]

#filter out property types
df_syd <- df_syd %>%
  filter(property_type %in% c("Entire rental unit", "Entire residential home", "Private room in rental unit", "Private room in residential home", "Entire serviced apartment"))

```

## Analysis
Before the analysis, I checked the price of the apartments variables (also viewed its logarithm form) for extreme values with a histogram graph I did not find many, which is better for the analysis. I also checked the price of each room type. The boxplot results show the price range of each room type. The boxplot graph shows, what can I expect for the resulting price at the end of each part of the analysis. The "entire home/apartments" are around 310 US dollars, but those apartments, which are in the private room type are around 750 US dollars.

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 3, fig.align="center" }

# price
ggplot(data=df_syd, aes(x=price)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)),
                  fill = 'navyblue', color = 'white', show.legend=F,  na.rm=TRUE) +
  labs(x = "Price (US dollars)",y = "Percent")+
  scale_x_continuous(expand = c(0.00,0.00),limits=c(100,1000), breaks = seq(100,1000, 200)) +
  theme_bw() 

# lnprice
ggplot(data=df_syd, aes(x=ln_price)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 0.18,
                 color = 'white', fill = 'navyblue', show.legend=F,  na.rm=TRUE) +
scale_y_continuous(expand = c(0.00,0.00),limits=c(0, 0.15), breaks = seq(0, 0.15, by = 0.05), labels = scales::percent_format(5L)) +
  labs(x = "ln(price, US dollars)",y = "Percent")+
  theme_bw() 

## Boxplot of price by room type
ggplot(data = df_syd, aes(x = f_room_type, y = price)) +
  stat_boxplot(aes(group = f_room_type), geom = "errorbar", width = 0.3,
               color = c('red','blue'), size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = f_room_type),
               color = c('red','blue'), fill = c('red','blue'),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  labs(x = "Room type",y = "Price (US dollars)")+
  theme_bw()


```

The training dataset I created has 8695 observations, which is better for the model estimation, and the holdout dataset with 3724 observations is still well enough for performance evaluation.
For the analysis, I created 3 predictors: The first one has only the basic variables, which contain only variables, which can have a major effect on apartment prices. The second predictor contains the basic variables, along with the variables about reviews, and the dummy variables I created as extra. The last predictor has extra observations for the LASSO model.
For the first part of my analysis, I started with random forests. To tune all the models, I used five-fold cross-validation everywhere, and eight variables at the random forest for each split. I used the 'variance' method, for optimizing the Mean Squared Error.

```{r, echo=FALSE, include=FALSE}

to_drop <- c("room-darkening_shades", "have_o_machine|ee_machine|coffee", "have_free.*street", "have_free.*on_premises", "have_wifi|internet")
df_syd <- df_syd%>%
  select(-one_of(to_drop))

# basic descr stat
skimr::skim(df_syd)
datasummary(price~Mean+Median+P25+P75+N,data=df_syd)
datasummary( f_room_type + property_type ~ N + Percent() , data = df_syd )

set.seed(2801)
train_i <- as.integer(createDataPartition(df_syd$price, p = 0.7, list = FALSE))
data_train <- df_syd[train_i, ]
data_holdout <- df_syd[-train_i, ]

# Check the number of observations
dim(data_train)
dim(data_holdout)

# Basic Variables
basic_vars <- c("accommodates", "beds", "no_day_since_r", "property_type","f_room_type", "bedrooms", "neighbourhood_cleansed", "minimum_nights", "maximum_nights", "beds", "no_day_since_r", "host_listings_count", "calculated_host_listings_count", "n_host_response_rate")

# reviews
reviews <- c("number_of_reviews" , "number_of_reviews_ltm", "reviews_per_month", "review_scores_value", "review_scores_cleanliness", "review_scores_rating", "review_scores_location", "availability_365")

# dummy
dummies <- c("instant_bookable_t", "has_availability_t", "host_identity_verified_t", "host_is_superhost_t", "host_resp_in_h_d", "host_resp_in_f_hour_d", "host_resp_in_days_d", "host_resp_days_d")

#interactions for the LASSO
X1  <- c("accommodates*property_type",  "f_room_type*property_type",  "f_room_type*instant_bookable_t",
         "has_availability_t*property_type", "host_identity_verified_t*property_type",
         "host_is_superhost_t*property_type")
# with boroughs
X2  <- c("property_type*neighbourhood_cleansed", "f_room_type*neighbourhood_cleansed",
         "accommodates*neighbourhood_cleansed")


predictors_1 <- c(basic_vars)
predictors_2 <- c(basic_vars, dummies, reviews)
predictors_3 <- c(basic_vars, dummies, reviews, X1, X2)

```

The results of the two random forest models show quite similar conclusions: The Mean Absolute Error of the two models is almost the same. The second model is better around 1.2% (model_1 = 173.09 model_2 =170.9). The second random forest model has a slightly better result at RMSE as well. 1,54 dollars better RMSE at the second model (model_1 = 212.59 model_2 = 211.05). R2 shows the same as well: The second model fits better by 0.7%, so I used this second model for later analysis.

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 3, fig.align="center" }

# do 5-fold CV
train_control <- trainControl(method = "cv",
                              number = 5,
                              verboseIter = FALSE)

# set tuning
tune_grid <- expand.grid(
  .mtry = c(8),
  .splitrule = "variance",
  .min.node.size = c(50)
)


# simpler model for model - using random forest
set.seed(1234)
system.time({
rf_model_1 <- train(
  formula(paste0("price ~", paste0(predictors_1, collapse = " + "))),
  data = data_train,
  method = "ranger",
  trControl = train_control,
  tuneGrid = tune_grid,
  importance = "impurity"
)
})

# more complicated model - using random forest
set.seed(1234)
system.time({
rf_model_2 <- train(
  formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
  data = data_train,
  method = "ranger",
  trControl = train_control,
  tuneGrid = tune_grid,
  importance = "impurity"
)
})

# evaluate random forests
results <- resamples(
  list(
    model_1  = rf_model_1,
    model_2  = rf_model_2))

summary(results)

```
After the foundation of the better model for the dataset, I started to check the variable importance. I plotted them with the help of the ranger package. To improve the Mean Squared Error I scaled back model_2 by 1000. The results show that the first 4 most influential variables on price are related to Room type: Private (almost 20%) and Property type: Entire, Private rental units (6% - 17%). The rest of the TOP 10 most influential variables were 2.5% and 4%. For the sake of the analysis, I considered that there is no causal link between the Top 10 most influential variables and the price of the apartments. This graph showed the direction, where should I start my investigation.


```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 3, fig.align="center" }

rf_model_2_var_imp <- ranger::importance(rf_model_2$finalModel)/1000
rf_model_2_var_imp_df <-
  data.frame(varname = names(rf_model_2_var_imp),imp = rf_model_2_var_imp) %>%
  mutate(varname = gsub("neighbourhood_cleansed", "District:", varname) ) %>%
  mutate(varname = gsub("f_room_type", "Room type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))

# have a version with top 10 vars only
ggplot(rf_model_2_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.75) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()

```

After the grouping is applied to the variables, the graph shows a much clearer picture of the importance.
Property Type has the highest calculated importance (almost 45%). Room type and Neighbourhood have much less important in terms of price (25% and 15%). The interesting part for me is that the number of accommodates has only around 5%. Before the analysis, my prediction was that the variable accommodates will have very high importance.

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 3, fig.align="center" }

# grouped variable importance - keep binaries created off factors together
varnames <- rf_model_2$finalModel$xNames
f_neighbourhood_varnames <- grep("neighbourhood_cleansed",varnames, value = TRUE)
f_room_type_varnames <- grep("f_room_type",varnames, value = TRUE)
f_property_type_varnames <- grep("property_type",varnames, value = TRUE)

groups <- list(f_neighbourhood=f_neighbourhood_varnames,
               f_room_type = f_room_type_varnames,
               bedrooms = "bedrooms",
               f_property_type = f_property_type_varnames,
               no_day_since_r = "no_day_since_r",
               accommodates = "accommodates",
               beds = "beds")

# Need a function to calculate grouped varimp
group.importance <- function(rf.obj, groups) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(ranger::importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}

rf_model_2_var_imp_grouped <- group.importance(rf_model_2$finalModel, groups)
rf_model_2_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_2_var_imp_grouped),
                                            imp = rf_model_2_var_imp_grouped[,1])  %>%
                                      mutate(imp_percentage = imp/sum(imp))

ggplot(rf_model_2_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.7) +
  ylab("Importance (Percent)") +   xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()

```

At this part of the analysis, I wanted to predict the price of apartments at different variables and visualize them. I used only the second random forest model since it was more sophisticated. The prediction of these models is based on the data_train sample, however, the expected values are coming from the data_holdout sample.
In the first graph, I predicted the price of apartments in different neighborhoods. The results show that in most cases the prices are not so volatile. The predicted prices for most cases move between 475 and 495 US dollars.
The second graph shows the predicted price for different room types. By the room types, the predicted prices are much more volatile. The Private rooms (530 US dollars) are 80 US dollars more expensive than the Entire homes and apartments.
The last graph shows the predicted prices of apartments by the number of accommodates. The accommodates variable again shows interesting results: The predicted values for 3 and 4 people are the cheapest, an apartment for two people is more expensive than a one with a place for five people. Maybe this trend is because apartments for two people are aiming to be more luxurious.

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 3, fig.align="center" }

# Partial Dependence Plots

# 1) neighbourhood
pdp_n_nei <- pdp::partial(rf_model_2, pred.var = "neighbourhood_cleansed", 
                        pred.grid = distinct_(data_holdout, "neighbourhood_cleansed"), 
                        train = data_train)

pdp_n_nei %>%
autoplot( ) +
  geom_point(color='red', size=2) +
  geom_line(color='red', size=1) +
  ylab("Predicted price") +
  xlab("Neiborhood") +
  theme(axis.text.x = element_text (angle = 40, hjust=1)) +
theme_bw()

# 2) Room type
pdp_n_roomtype <- pdp::partial(rf_model_2, pred.var = "f_room_type", 
                               pred.grid = distinct_(data_holdout, "f_room_type"), 
                               train = data_train)
pdp_n_roomtype %>%
  autoplot( ) +
  geom_point(color='red', size=4) +
  ylab("Predicted price") +
  xlab("Room type") +
  theme_bw()


# 3) Number of accommodates
pdp_n_acc <- pdp::partial(rf_model_2, pred.var = "accommodates", 
                          pred.grid = distinct_(data_holdout, "accommodates"), 
                          train = data_train)

pdp_n_acc %>%
  autoplot( ) +
  geom_point(color='red', size=2) +
  geom_line(color='red', size=1) +
  ylab("Predicted price") +
  xlab("Accommodates (persons)") +
  scale_x_continuous(limit=c(1,7), breaks=seq(1,7,1))+
theme_bw()


```

The next part of the analysis is about checking five different methods' performance compare with each other. For this analysis, I used the data_holdout sample. As the first step, I predicted the prices of the apartments in the holdout dataset. I check that price on different models, how it performs. 
The RMSE cannot be used by the comparison, since it is in Us dollars. As a solution, I divided RMSE with the mean value of price. The reason for that is to have a value I can use to compare the models.
The results show that there is a big difference between small and large apartments in terms of predicting power (around 13%). The case is similar by the Room type: Entire homes and apartments are 11.5% worse than Private rooms, so the model performs much better to predict Private room prices.
By the neighborhood variable, the performance of the different neighborhoods is quite similar, so there is no better district in terms of predicting performance.

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 3, fig.align="center" }

# ---- cheaper or more expensive flats
data_holdout_w_prediction <- data_holdout %>%
  mutate(predicted_price = predict(rf_model_2, newdata = data_holdout))



######### create nice summary table of heterogeneity
a <- data_holdout_w_prediction %>%
  mutate(is_low_size = ifelse(accommodates <= 3, "small apt", "large apt")) %>%
  group_by(is_low_size) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = RMSE(predicted_price, price) / mean(price)
  )


b <- data_holdout_w_prediction %>%
  filter(neighbourhood_cleansed %in% c("Woollahra", "Sydney", "Auburn", "Randwick",
                                         "Waverley", "City Of Kogarah", "Campbelltown",
                                         "Hornsby", "North Sydney")) %>%
  group_by(neighbourhood_cleansed) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = rmse / mean_price
  )

c <- data_holdout_w_prediction %>%
  filter(f_room_type %in% c("Private room", "Entire home/apt")) %>%
  group_by(f_room_type) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = rmse / mean_price
  )


d <- data_holdout_w_prediction %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = RMSE(predicted_price, price) / mean(price)
  )


colnames(a) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(b) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(c) <- c("", "RMSE", "Mean price", "RMSE/price")
d<- cbind("All", d)
colnames(d) <- c("", "RMSE", "Mean price", "RMSE/price")

line1 <- c("Type", "", "", "")
line2 <- c("Apartment size", "", "", "")
line3 <- c("District", "", "", "")

result_3 <- rbind(line2, a, line1, c, line3, b, d) %>%
  transform(RMSE = as.numeric(RMSE), `Mean price` = as.numeric(`Mean price`),
            `RMSE/price` = as.numeric(`RMSE/price`))

result_3


```

 The last part of this analysis compares different prediction methods to see, which method gives the best possible results. It is important to check many methods since a company will set the prices of its apartments. The company's profit is hugely affected by this analysis and prediction. To not make any mistakes, I checked many methods to find the best solution.
The first model is an OLS and the second predictor model was used for the calculation. The second model is the LASSO model, which used the third predictor model with the interactions. The last method is CART, where again the second model was used. The difference at the last method: the model is tuned for 10 complexity parameters to chose from, using only cross-validation.
A boosting model was also used for better results. The GBM model was tuned for 250 pieces of 5 layers deep tree building, shrinkage is 0.1 and the minimum number of observations is 20.

As a result of the comparison, we can see that there is not much of a difference between the best and the worst model. The difference is only around 3%. The comparison on the holdout dataset shows similar results: The rank of the models in terms of performance is the same, only the difference between them is larger, almost 5%.

As the result of the current analysis, I think the company, which rents apartments on Airbnb, should focus GBM model with extended variables. My decision is based on, GBMt model has the highest performance in predictions. In the future, they can use this model easier, faster, and more accurately compared to other models to check their prices. 
The GBM and the second Random Forest performed equally well. These models are followed by CART and then LASSO model. The last contender is OLS. The best model, estimate it on the work set, and evaluate it on the holdout set. The best model is GBM and its holdout set RMSE is 210.2. The expectation is to make an error of 210 US dollars when using the model on the live data. The holdout set RMSE is quite close to the cross-validated RMSE.
So, the company should consider the RMSE, when they set their models to predict their apartments prices. The RMSE is quite huge and the R2 is also not so good. The analysis could be better, if I could generate binary variables from the amenities variable.

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 3, fig.align="center" }

# OLS

set.seed(1234)
system.time({
ols_model <- train(
  formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
  data = data_train,
  method = "lm",
  trControl = train_control
)
})

ols_model_coeffs <-  ols_model$finalModel$coefficients
ols_model_coeffs_df <- data.frame(
  "variable" = names(ols_model_coeffs),
  "ols_coefficient" = ols_model_coeffs
) %>%
  mutate(variable = gsub("`","",variable))

# LASSO
set.seed(1234)
system.time({
lasso_model <- train(
  formula(paste0("price ~", paste0(predictors_3, collapse = " + "))),
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 0.25, by = 0.01)),
  trControl = train_control
)
})

lasso_coeffs <- coef(
    lasso_model$finalModel,
    lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(lasso_coefficient = `s1`)

# CART with built-in pruning
set.seed(1234)
system.time({
cart_model <- train(
  formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
  data = data_train,
  method = "rpart",
  tuneLength = 10,
  trControl = train_control
)
})

# boosting models
gbm_grid <-  expand.grid(interaction.depth = 5,
                         n.trees = 250,
                         shrinkage = 0.1,
                         n.minobsinnode = 20
)


set.seed(1234)
system.time({
  gbm_model <- train(formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
                     data = data_train,
                     method = "gbm",
                     trControl = train_control,
                     verbose = FALSE,
                     tuneGrid = gbm_grid)
})


final_models <-
  list("OLS" = ols_model,
  "LASSO (model w/ interactions)" = lasso_model,
  "CART" = cart_model,
  "Random forest 1: smaller model" = rf_model_1,
  "Random forest 2: extended model" = rf_model_2,
  "GBM"  = gbm_model)

results <- resamples(final_models) %>% summary()
results

# Model selection is carried out on this CV RMSE
result_4 <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV RMSE" = ".")

result_4


result_5 <- map(final_models, ~{
  RMSE(predict(.x, newdata = data_holdout), data_holdout[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("Holdout RMSE" = ".")

result_5

```
